{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_COL = \"embeddings\"\n",
    "TEXT_COL = \"text\"\n",
    "WEIGHT_COL = \"weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Column(object):\n",
    "    def __init__(self, name, attribute, col_type, verb):\n",
    "        self.name = name\n",
    "        self.attribute = attribute\n",
    "        self.type = col_type\n",
    "        self.verb = verb\n",
    "        \n",
    "    def is_binary(self):\n",
    "        return self.type == \"binary\"\n",
    "    \n",
    "    def is_categorical(self):\n",
    "        return self.type == \"categorical\"\n",
    "    \n",
    "    def is_numerical(self):\n",
    "        return self.type == \"numerical\"\n",
    "    \n",
    "    def create_sentence(self, value, prefix, missing_word, replace_numbers, descriptive):\n",
    "        if descriptive:\n",
    "            return self.create_descriptive_sentence(value, prefix, missing_word, replace_numbers)\n",
    "        else:\n",
    "            return self.create_basic_sentence(value, prefix, missing_word, replace_numbers)\n",
    "        \n",
    "        \n",
    "class Binary_Column(Column):\n",
    "    def __init__(self, name, attribute, verb, neg_verb):\n",
    "        self.neg_verb = neg_verb\n",
    "        super().__init__(name, attribute, \"binary\", verb)\n",
    "        \n",
    "\n",
    "    def create_descriptive_sentence(self, value, prefix, missing_word, replace_numbers):\n",
    "        sentence = \"\"\n",
    "        if str(value).lower()  in [\"1\", \"0\", \"true\", \"false\"]:\n",
    "            if int(value) == 1:\n",
    "                sentence = prefix + self.verb + \" \" + self.attribute\n",
    "            elif int(value) == 0:\n",
    "                sentence = prefix + self.neg_verb + \" \" + self.attribute\n",
    "        return sentence\n",
    "            \n",
    "\n",
    "    def create_basic_sentence(self, value, prefix, missing_word, replace_numbers):\n",
    "        sentence = \"\"\n",
    "        if str(value).lower()  in [\"1\", \"0\", \"true\", \"false\"]:\n",
    "            if int(value) == 1:\n",
    "                sentence = self.verb + \" \" + self.attribute + \": yes\" \n",
    "            elif int(value) == 0:\n",
    "                sentence = self.neg_verb + \" \" + self.attribute +\" : no\"\n",
    "        elif missing_word != \"\":\n",
    "            sentence = self.verb + \" \" + self.attribute + \": \" + missing_word\n",
    "        return sentence\n",
    "        \n",
    "class Categorical_Column(Column):\n",
    "    def __init__(self, name, attribute, verb):\n",
    "        super().__init__(name, attribute, \"categorical\", verb)\n",
    "\n",
    "    def create_descriptive_sentence(self, value, prefix, missing_word, replace_numbers):\n",
    "        if len(prefix) != 0:\n",
    "            prefix = prefix[:-1] + \"'s \"\n",
    "        sentence = \"\"\n",
    "        if str(value).lower() not in [\"nan\", \"\", \"none\", \"missing\"]:\n",
    "            sentence = prefix + self.attribute + \" \" + self.verb + \" \" + str(value)\n",
    "        elif  missing_word != \"\":\n",
    "            sentence = prefix + self.attribute + \" \" + self.verb + \" \" + missing_word\n",
    "        return sentence\n",
    "            \n",
    "\n",
    "    def create_basic_sentence(self, value, prefix, missing_word, replace_numbers):\n",
    "        if len(prefix) != 0:\n",
    "            prefix = prefix[:-1] + \"'s \"\n",
    "        sentence = \"\"\n",
    "        if  str(value).lower() not in [\"nan\", \"\", \"none\", \"missing\"]:\n",
    "            sentence = self.attribute + \": \" + str(value)\n",
    "        elif missing_word != \"\":\n",
    "            sentence = self.attribute + \": \" + missing_word\n",
    "        return sentence\n",
    "    \n",
    "class Numerical_Column(Column):\n",
    "    def __init__(self, name, attribute, verb, avg, sd):\n",
    "        self.avg = avg\n",
    "        self.sd = sd\n",
    "        super().__init__(name, attribute, \"numerical\", verb)\n",
    "        \n",
    "        \n",
    "    def create_descriptive_sentence(self, value, prefix, missing_word, replace_numbers):\n",
    "        if len(prefix) != 0:\n",
    "            prefix = prefix[:-1] + \"'s \"\n",
    "        sentence = \"\"\n",
    "        if str(value).lower() not in [\"nan\", \"\", \"none\", \"missing\"]:\n",
    "            col_value = self.encode_number(value, replace_numbers)\n",
    "            sentence = prefix + self.attribute + \" \" + self.verb + \" \" + str(col_value) \n",
    "        elif  missing_word != \"\":\n",
    "            sentence = prefix + self.attribute + \" \" + self.verb + \" \" + missing_word \n",
    "        return sentence\n",
    "            \n",
    "\n",
    "    def create_basic_sentence(self, value, prefix, missing_word, replace_numbers):\n",
    "        if len(prefix) != 0:\n",
    "            prefix = prefix[:-1] + \"'s \"\n",
    "        sentence = \"\"\n",
    "        if  str(value).lower() not in [\"nan\", \"\", \"none\", \"missing\"]:\n",
    "            col_value = self.encode_number(value, replace_numbers)\n",
    "            sentence = self.attribute + \": \" + str(col_value)\n",
    "        elif missing_word != \"\":\n",
    "            sentence = self.attribute + \": \" + missing_word\n",
    "        return sentence\n",
    "    \n",
    "    def encode_number(self, value, replace_numbers):\n",
    "        new_value = value\n",
    "        if replace_numbers:\n",
    "            if self.avg - 2*self.sd > value:\n",
    "                new_value = \"very low\"\n",
    "            elif self.avg - 2*self.sd <= value < self.avg - self.sd:\n",
    "                new_value = \"low\"\n",
    "            elif self.avg + 2*self.sd >= value > self.avg + self.sd:\n",
    "                new_value = \"high\"\n",
    "            elif self.avg + 2*self.sd < value:\n",
    "                new_value = \"very high\"\n",
    "            else:\n",
    "                new_value = \"normal\"\n",
    "        return new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_col = Binary_Column(\"hisp\", \"hispanic\", \"is\", \"is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the patient is hispanic'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_col.create_descriptive_sentence(\"1\", \"the patient \",\"is missing\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = Categorical_Column(\"nat\", \"nationality\", \"is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = Numerical_Column(\"tmp\", \"temperature\",  \"is\", 10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import isnan\n",
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#One table per patient per tabular data structure\n",
    "class Table(object):\n",
    "    def __init__(self, name, df, columns, metadata, time_col):\n",
    "        self.name = name\n",
    "        self.headers = df.columns\n",
    "        self.columns = columns\n",
    "        self.metadata = metadata\n",
    "        self.df = df\n",
    "        self.time_col = time_col\n",
    "\n",
    "    def is_temporal(self):\n",
    "        return self.time_col is not None\n",
    "    \n",
    "    def is_static(self):\n",
    "        return self.time_col is None\n",
    "\n",
    "        \n",
    "    def create_text(self, prefix, missing_word, replace_numbers, descriptive):\n",
    "        text = []\n",
    "        for t_i in range(self.df.shape[0]):\n",
    "            text_i = self.metadata\n",
    "            \n",
    "            for column in self.columns:\n",
    "                print\n",
    "                value = self.df.iloc[t_i][column.name]\n",
    "                col_text = column.create_sentence(value, prefix, missing_word, replace_numbers, descriptive)\n",
    "                if len(col_text) >0:\n",
    "                    col_text += \", \"\n",
    "                text_i += col_text\n",
    "                \n",
    "            text.append(text_i)\n",
    "    \n",
    "        self.df[TEXT_COL] = text \n",
    "\n",
    "    \n",
    "    def create_embeddings(self):\n",
    "        embeddings = []\n",
    "\n",
    "        for i in range(self.df.shape[0]):\n",
    "            text = self.df.iloc[i][TEXT_COL]\n",
    "            full_embedding = get_biobert_embeddings(text)[0]\n",
    "            embeddings.append(full_embedding.reshape(-1))\n",
    "\n",
    "        self.df[EMB_COL] = embeddings\n",
    "        \n",
    "    def get_timebounded_df(self, start_hr, end_hr):\n",
    "        \n",
    "        if self.time_col is None:\n",
    "            return self.df\n",
    "        \n",
    "        else:\n",
    "            timebounded_df = self.df.copy()\n",
    "\n",
    "            if start_hr is not None:\n",
    "                timebounded_df = timebounded_df[timebounded_df[time_col]>= start_hr]\n",
    "            if end_hr is not None:\n",
    "                timebounded_df = timebounded_df[timebounded_df[time_col]<= end_hr]\n",
    "\n",
    "            return timebound_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from functools import reduce\n",
    "\n",
    "class Patient(object):\n",
    "    def __init__(self, tables, pat_id, time_col):\n",
    "        self.id = pat_id\n",
    "        self.time_col = time_col   \n",
    "        self.tables = tables  \n",
    "        \n",
    "    def get_tables_name(self):\n",
    "        table_names = []\n",
    "        for table in self.tables:\n",
    "            table_names.append(table.name)\n",
    "        return table_names\n",
    "\n",
    "        \n",
    "    def create_timed_data(self, prefix, missing_word, replace_numbers, descriptive, merge_tables_text=True):\n",
    "        for table in self.tables:\n",
    "            table.create_text(prefix, missing_word, replace_numbers, descriptive)\n",
    "            \n",
    "        timed_data = reduce(lambda t1, t2: merge_text(t1, t2, self.time_col), self.tables).df\n",
    "        \n",
    "        if merge_tables_text:\n",
    "            timed_data[EMB_COL] = create_embeddings(timed_data)\n",
    "        \n",
    "        else:\n",
    "            for table in self.tables:\n",
    "                table.create_embeddings()\n",
    "\n",
    "            emb_data = reduce(lambda t1, t2: merge_emb(t1, t2, self.time_col), self.tables).df\n",
    "            timed_data[EMB_COL + \"_per_table\"] = emb_data[EMB_COL]\n",
    "        self.timed_data = timed_data\n",
    "        return timed_data\n",
    "    \n",
    "\n",
    "    def get_timebounded_embeddings(self, weight_fn, start_hr = None, end_hr = None, merge_tables_text=True):\n",
    "        timebounded_df = self.timed_data.copy()\n",
    "        \n",
    "        if merge_tables_text:\n",
    "            timebounded_df = timebounded_df[[EMB_COL, self.time_col]]\n",
    "        else:\n",
    "            timebounded_df = timebounded_df[[EMB_COL + \"_per_table\", self.time_col]]\n",
    "            \n",
    "        if start_hr is not None:\n",
    "            timebounded_df = timebounded_df[timebounded_df[self.time_col]>= start_hr]\n",
    "        if end_hr is not None:\n",
    "            timebounded_df = timebounded_df[timebounded_df[self.time_col]<= end_hr]\n",
    "\n",
    "        timebounded_df[WEIGHT_COL] = weight_fn(timebounded_df[self.time_col])\n",
    "        return timebounded_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Patient Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patients(tables_info, id_col, time_col):\n",
    "    unique_ids = get_unique_ids(tables_info, id_col)\n",
    "    patients = []\n",
    "    for pat_id in unique_ids:\n",
    "        tables = create_patient_tables(tables_info, pat_id, id_col, time_col)\n",
    "        patient = Patient(tables, pat_id, time_col)\n",
    "        patients.append(patient)\n",
    "    return patients\n",
    "    \n",
    "def create_columns(attributes_info):\n",
    "    columns = []\n",
    "    for col_name in attributes_info:\n",
    "        col_attribute = attributes_info[col_name][\"attribute\"]\n",
    "        col_verb = attributes_info[col_name][\"column_verb\"]\n",
    "        col_type = attributes_info[col_name][\"column_type\"]\n",
    "        if col_type == \"binary\":\n",
    "            col_neg_verb = attributes_info[col_name][\"column_neg_verb\"]\n",
    "            column = Binary_Column(col_name, col_attribute, col_verb, col_neg_verb)\n",
    "        elif col_type == \"categorical\":\n",
    "            column = Categorical_Column(col_name, col_attribute, col_verb)\n",
    "        else:\n",
    "            avg = attributes_info[col_name][\"avg\"]\n",
    "            sd = attributes_info[col_name][\"sd\"]\n",
    "            column = Numerical_Column(col_name, col_attribute, col_verb, avg, sd)\n",
    "        columns.append(column)\n",
    "    return columns\n",
    "\n",
    "def create_patient_tables(tables_info, pat_id, id_col, time_col):\n",
    "    pat_tables = []\n",
    "    for i in range(len(tables_info)):\n",
    "        \n",
    "        table_df = tables_info[i][\"df\"]\n",
    "        table_name = tables_info[i][\"name\"]\n",
    "        attributes_info = tables_info[i][\"attributes_info\"]\n",
    "        columns = create_columns(attributes_info)\n",
    "        metadata = tables_info[i][\"metadata\"]\n",
    "        \n",
    "        if pat_id in table_df[id_col].unique():\n",
    "            pat_table_df = table_df[table_df[id_col]== pat_id]\n",
    "            if time_col not in table_df.columns:\n",
    "                table = Table(table_name, pat_table_df, columns, metadata, None)\n",
    "            else:\n",
    "                table = Table(table_name, pat_table_df, columns, metadata, time_col)\n",
    "            pat_tables.append(table)\n",
    "    return pat_tables\n",
    "\n",
    "def get_unique_ids(tables_info, id_key):\n",
    "    unique_ids = set()\n",
    "    for i in range(len(tables_info)):\n",
    "        table_df = tables_info[i][\"df\"]\n",
    "        table_ids = table_df[id_key].unique()\n",
    "        unique_ids.update(table_ids)\n",
    "    return unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/gridsan/kimvc/haim_shared/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import isnan\n",
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "biobert_path = '/home/gridsan/kimvc/haim_shared/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/'\n",
    "biobert_tokenizer = AutoTokenizer.from_pretrained(biobert_path)\n",
    "biobert_model = AutoModel.from_pretrained(biobert_path)\n",
    "\n",
    "def get_biobert_embeddings(text):\n",
    "    # Inputs:\n",
    "    #   text -> Input text (str)\n",
    "    #\n",
    "    # Outputs:\n",
    "    #   embeddings -> Final Biobert embeddings with vector dimensionality = (1,768)\n",
    "    #   hidden_embeddings -> Last hidden layer in Biobert model with vector dimensionality = (token_size,768)\n",
    "\n",
    "    # %% EXAMPLE OF USE\n",
    "    # embeddings, hidden_embeddings = get_biobert_embeddings(text)\n",
    "\n",
    "    tokens_pt = biobert_tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = biobert_model(**tokens_pt)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooler_output = outputs.pooler_output\n",
    "    hidden_embeddings = last_hidden_state.detach().numpy()\n",
    "    embeddings = pooler_output.detach().numpy()\n",
    "\n",
    "    return embeddings, hidden_embeddings\n",
    "\n",
    "def create_embeddings(df):\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        text = df.iloc[i][TEXT_COL]\n",
    "        full_embedding = get_biobert_embeddings(text)[0]\n",
    "        embeddings.append(full_embedding.reshape(-1))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def create_time_weights(timestamps):\n",
    "    #TO-DO\n",
    "    n = len(timestamps)\n",
    "    return [1/n for i in range(n)]\n",
    "\n",
    "def merge_text(table1, table2, time_col):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_time_col = time_col\n",
    "    if table1.is_static() and table2.is_static():\n",
    "        df = table1.df.copy()\n",
    "        df[TEXT_COL] = table1.df[TEXT_COL] + table2.df[TEXT_COL]\n",
    "        new_df = df[TEXT_COL]\n",
    "        new_time_col = None\n",
    "    elif table1.is_static():\n",
    "        df = table2.df.copy()\n",
    "        df[TEXT_COL] = table1.df.iloc[0][TEXT_COL] + table2.df[TEXT_COL]\n",
    "        new_df = df[[time_col, TEXT_COL]]\n",
    "    elif table2.is_static():\n",
    "        df = table1.df.copy()\n",
    "        df[TEXT_COL] = table1.df[TEXT_COL] + table2.df.iloc[0][TEXT_COL]\n",
    "        new_df = df[[time_col, TEXT_COL]]\n",
    "    else:\n",
    "        df = table1.df.copy()\n",
    "        df = df.merge(table2.df, how=\"outer\", on=time_col)\n",
    "        print()\n",
    "        df = df.fillna(\"\")\n",
    "        df[TEXT_COL] = df[TEXT_COL + \"_x\"] + df[TEXT_COL + \"_y\"]\n",
    "        new_df = df[[time_col, TEXT_COL]]\n",
    "    table = Table(\"Merged Table\", new_df, table1.columns + table2.columns, \"\", new_time_col)\n",
    "    return table\n",
    "\n",
    "    \n",
    "def merge_emb(table1, table2, time_col):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_time_col = time_col\n",
    "    if table1.is_static() and table2.is_static():\n",
    "        df = table1.df.copy()\n",
    "        df[EMB_COL] = np.concatenate((df1[EMB_COL][0] ,  df2[EMB_COL][0]))\n",
    "        new_df = df[EMB_COL]\n",
    "        new_time_col = None\n",
    "    elif table1.is_static():\n",
    "        df = table2.df.copy()\n",
    "        df[EMB_COL] = df[EMB_COL].apply(lambda x: np.concatenate((table1.df.iloc[0][EMB_COL], x)))\n",
    "        new_df = df[[time_col, EMB_COL]]\n",
    "    elif table2.is_static():\n",
    "        df = table1.df.copy()\n",
    "        df[EMB_COL] = df[EMB_COL].apply(lambda x: np.concatenate((x, table2.df.iloc[0][EMB_COL])))\n",
    "        new_df = df[[time_col, EMB_COL]]\n",
    "    else:\n",
    "        df = table1.df.copy()\n",
    "        df = df.merge(table2.df, how=\"outer\", on=time_col)\n",
    "        df[EMB_COL + \"_x\"] = df[EMB_COL + \"_x\"].apply(lambda d: d if isinstance(d, np.ndarray) else [])\n",
    "        df[EMB_COL + \"_y\"] = df[EMB_COL + \"_y\"].apply(lambda d: d if isinstance(d, np.ndarray) else [])\n",
    "        df[EMB_COL] = [np.concatenate((df[EMB_COL + \"_x\"][i], df[EMB_COL + \"_y\"][i])) for i in range(len(df))]\n",
    "        new_df = df[[time_col, EMB_COL]]\n",
    "    table = Table(\"Merged Table\", new_df, table1.columns + table2.columns, \"\", new_time_col)\n",
    "    return table\n",
    "    \n",
    "def get_attributes_info(df, info_file_path):\n",
    "    attributes_info = {}\n",
    "    info_file = pd.read_csv(info_file_path)\n",
    "    for i in range(info_file.shape[0]):\n",
    "        col_name, attribute, verb, neg_verb, col_type = info_file.iloc[i]\n",
    "        sd, avg = None, None\n",
    "        if col_type == \"numerical\":\n",
    "            col_values = df[[col_name]].astype(np.float)\n",
    "            col_values = col_values[col_name][pd.notnull(col_values[col_name])]\n",
    "            avg = col_values.mean()\n",
    "            sd = col_values.std()\n",
    "        attributes_info[col_name] = {\"attribute\": attribute,\n",
    "                                    \"column_verb\": verb,\n",
    "                                    \"column_neg_verb\": neg_verb,\n",
    "                                    \"column_type\": col_type,\n",
    "                                    \"avg\": avg,\n",
    "                                    \"sd\": sd}\n",
    "    return attributes_info\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_RASS_LAST(rass_col):\n",
    "    new_rass = []\n",
    "    for i in range(len(rass_col)):\n",
    "        if str(rass_col[i]).lower() in [\"nan\", \"none\", \"\"]:\n",
    "            new_rass.append(str(rass_col[i]))\n",
    "        else:\n",
    "            new_rass.append(rass_col[i].split(\">\")[1])\n",
    "    return new_rass\n",
    "\n",
    "def fix_ipa_ambulating(ambulating_col):\n",
    "    new_amb = []\n",
    "    amb_map = {0:\"not difficult\", 1:\"moderately difficult\", 2:\"cannot ambulate independentely\"}\n",
    "    for i in range(len(ambulating_col)):\n",
    "        if str(ambulating_col[i]).lower() in [\"nan\", \"none\", \"\"]:\n",
    "            new_amb.append(str(ambulating_col[i]))\n",
    "        else:\n",
    "            new_amb.append(amb_map[float(ambulating_col[i])])\n",
    "    return new_amb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2021a/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2021a/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2021a/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "event_df = pd.read_csv(\"../allData_downloaded/adtEventData.csv\", \"|\")\n",
    "clin_df = pd.read_csv(\"../allData_downloaded/clindocData.csv\", \"|\")\n",
    "enc_df = pd.read_csv(\"../allData_downloaded/encounterData.csv\", \"|\")\n",
    "\n",
    "event_df = event_df[(event_df['PAT_ENC_CSN_ID'] == 100087860068) | (event_df['PAT_ENC_CSN_ID'] ==100083064488)]\n",
    "clin_df = clin_df[(clin_df['PAT_ENC_CSN_ID'] == 100087860068) | (clin_df['PAT_ENC_CSN_ID'] ==100083064488)]\n",
    "enc_df = enc_df[(enc_df['PAT_ENC_CSN_ID'] == 100087860068) | (enc_df['PAT_ENC_CSN_ID'] ==100083064488)]\n",
    "\n",
    "event_df['time'] = pd.to_datetime(event_df['EFFECTIVE_DTTM'], infer_datetime_format=True)#.dt.date \n",
    "clin_df['time'] = pd.to_datetime(clin_df['CALENDAR_DT'], infer_datetime_format=True)#.dt.date\n",
    "\n",
    "clin_df[\"RASS_LAST\"] = fix_RASS_LAST(clin_df[\"RASS_LAST\"].values)\n",
    "clin_df[\"IPA_DIFFICULTY_AMBULATING\"] = fix_ipa_ambulating(clin_df[\"IPA_DIFFICULTY_AMBULATING\"].values)\n",
    "\n",
    "events = {}\n",
    "events[\"df\"] = event_df\n",
    "events[\"name\"] = \"ADT_Events\"\n",
    "events[\"attributes_info\"] = get_attributes_info(event_df, \"adtEventDataColumnsInfo.csv\")\n",
    "events[\"metadata\"] = \"The following is the information for admission, discharge and transfer events. \"\n",
    "\n",
    "clinical = {}\n",
    "clinical[\"df\"] = clin_df\n",
    "clinical[\"name\"] = \"Clinical_Documents\"\n",
    "clinical[\"attributes_info\"] = get_attributes_info(clin_df, \"clinDocDataColumnsInfo.csv\")\n",
    "clinical[\"metadata\"] = \"The following is the clinical information. \"\n",
    "\n",
    "encounter = {}\n",
    "encounter[\"df\"] = enc_df\n",
    "encounter[\"name\"] = \"Encounter_Information\"\n",
    "encounter[\"attributes_info\"] = get_attributes_info(enc_df, \"encounterDataColumnsInfo.csv\")\n",
    "encounter[\"metadata\"] = \"The following is the encounter information. \"\n",
    "\n",
    "\n",
    "tables_info = [encounter, events, clinical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = get_patients(tables_info, 'PAT_ENC_CSN_ID', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_patient = patients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-222-5db081fdb209>:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df[TEXT_COL] = text\n",
      "<ipython-input-222-5db081fdb209>:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df[EMB_COL] = embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = first_patient.create_timed_data(\"the patient \", \"missing\", False, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeddings_per_table</th>\n",
       "      <th>time</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.18052378296852112, -0.15505649149417877, 0...</td>\n",
       "      <td>2020-01-07 19:43:00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.12036321312189102, -0.11608690768480301, 0...</td>\n",
       "      <td>2020-01-08 00:00:00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                embeddings_per_table                time  \\\n",
       "2  [-0.18052378296852112, -0.15505649149417877, 0... 2020-01-07 19:43:00   \n",
       "5  [-0.12036321312189102, -0.11608690768480301, 0... 2020-01-08 00:00:00   \n",
       "\n",
       "   weight  \n",
       "2     0.5  \n",
       "5     0.5  "
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_patient.get_timebounded_embeddings(create_time_weights, start_hr = t.iloc[2][\"time\"], end_hr = t.iloc[5][\"time\"], merge_tables_text=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
